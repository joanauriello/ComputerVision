{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "MCDAA, UdelaR\n",
        "# Computer Vision 2025 -  Proyecto Final - Labels and Data Augmentation\n",
        "Pablo Molina\n",
        "Joana Auriello"
      ],
      "metadata": {
        "id": "puiauTo4y-32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 2 ways of doing Augmentation. In this collab, we are doing OFFLINE data augmentation. But we have also applied on the fly data augmentation in our original notebook, here are the main differences and points:\n",
        "\n",
        "**1 — Offline Augmentation (Saving Augmented Images to Disk)**\n",
        "\n",
        "In offline augmentation, we physically generate new images from the original dataset and save each augmentation type—such as flip-only, rotation-only, zoom-only, contrast-only—into separate files. This creates a larger and fixed augmented dataset that can be reused for any model (ResNet, EfficientNet, YOLO) and inspected visually. The downside is that it is extremely time- and storage-intensive: for tens of thousands of images, saving multiple augmented versions can require many hours and hundreds of thousands of file writes to Google Drive. Offline augmentation also produces static variations—once images are saved, the model always sees the exact same augmented samples every epoch, reducing variability during training.\n",
        "\n",
        "**2 — Online Augmentation (ImageDataGenerator During Training)**\n",
        "\n",
        "With online augmentation using ImageDataGenerator (or Keras Random* layers), the model receives a new, randomly transformed version of each image every epoch, with no files saved to disk. This approach is far more efficient: it keeps the dataset size small and performs augmentation in memory, delivering endless variation over the course of training. For example, in 10 epochs, the model may effectively see 10 different augmented versions of each training image—creating far more diversity than offline augmentation while requiring almost no preprocessing time. This is the most common modern approach in deep learning pipelines because it is fast, flexible, and maximizes generalization without consuming storage."
      ],
      "metadata": {
        "id": "YLwC5XctOSPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Mount Google Drive, add labels to each image, save labeled images dataset to csv file"
      ],
      "metadata": {
        "id": "jhoZ6awGAvMh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9901a15"
      },
      "source": [
        "### 1. Mount Google Drive\n",
        "\n",
        "Run the following cell to mount your Google Drive. This will prompt you to authorize Colab to access your Drive files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9e57e70",
        "outputId": "bbf2a206-b62a-4bec-c932-822ff4339306"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# My working directory (small stuff: CSVs, logs, checkpoints)\n",
        "WORK_DIR = '/content/drive/MyDrive/Colab Notebooks/ComputerVision'\n",
        "\n",
        "# Pablo's directory (unlimited storage for big augmented dataset)\n",
        "PABLO_DIR = '/content/drive/MyDrive/[01] - Pablo/[01].[02] - Facultad/[01].[02].[05] - Master Fing/Computer Vision/Project/Entrega Reciclaje'\n",
        "\n",
        "# Original dataset location (folders like \"Bio organico\", \"Envase Plasticos\", etc.)\n",
        "RAW_DATA_DIR = os.path.join(WORK_DIR, 'Dataset Consolidado')\n",
        "\n",
        "print(\"WORK_DIR:\", WORK_DIR)\n",
        "print(\"PABLO_DIR:\", PABLO_DIR)\n",
        "print(\"RAW_DATA_DIR:\", RAW_DATA_DIR)\n",
        "\n",
        "# Quick check: list category folders\n",
        "!ls \"$RAW_DATA_DIR\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "WORK_DIR: /content/drive/MyDrive/Colab Notebooks/ComputerVision\n",
            "PABLO_DIR: /content/drive/MyDrive/[01] - Pablo/[01].[02] - Facultad/[01].[02].[05] - Master Fing/Computer Vision/Project/Entrega Reciclaje\n",
            "RAW_DATA_DIR: /content/drive/MyDrive/Colab Notebooks/ComputerVision/Dataset Consolidado\n",
            "'Bio organico'\t\t\t   'Reciclables Varios Metal'\n",
            " class_names.json\t\t   'Reciclables Varios Otros'\n",
            "'Envase Plasticos'\t\t   'Reciclables Varios Plastico'\n",
            " image_labels.csv\t\t   'Reciclables Varios Textiles'\n",
            "'Papel y Carton'\t\t   'Reciclables Varios Vidrio'\n",
            "'Reciclables Varios Cigarro'\t   'Todo lo demás'\n",
            "'Reciclables Varios Electronicos'   train_files.csv\n",
            "'Reciclables Varios Madera'\t    val_files.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23c97a02"
      },
      "source": [
        "### 2. Tag Original Dataset and Save image_labels.csv\n",
        "\n",
        "\n",
        "Here we scan the category folders, create a table with columns:\n",
        "\n",
        "image_path: full path to each image\n",
        "\n",
        "label: folder name (category)\n",
        "We save this as image_labels.csv so all models (ResNet, EfficientNet, YOLO) can reuse the same labels."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "# Find all image files in subfolders of RAW_DATA_DIR\n",
        "image_paths = glob.glob(os.path.join(RAW_DATA_DIR, '*', '*.*'))  # every file one level under each label folder\n",
        "\n",
        "data = []\n",
        "for path in image_paths:\n",
        "    label = os.path.basename(os.path.dirname(path))  # folder name = class label\n",
        "    data.append((path, label))\n",
        "\n",
        "df_all = pd.DataFrame(data, columns=['image_path', 'label'])\n",
        "\n",
        "print(\"Total images found:\", len(df_all))\n",
        "df_all.head()\n",
        "\n",
        "# Save tagged original dataset\n",
        "labels_csv_path = os.path.join(WORK_DIR, 'image_labels.csv')\n",
        "df_all.to_csv(labels_csv_path, index=False)\n",
        "print(\"Saved:\", labels_csv_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sw_gEaraB2Qv",
        "outputId": "17306ba0-e12b-44ff-84ab-2346e4a12ecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images found: 59001\n",
            "Saved: /content/drive/MyDrive/Colab Notebooks/ComputerVision/image_labels.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Encode Labels and Save class_names.json\n",
        "\n",
        "\n",
        "We create a stable mapping from label strings to integer IDs (label_id) and save the class list.\n",
        "This ensures all models use the same class ordering."
      ],
      "metadata": {
        "id": "Xa14q-E5Cfs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "df = pd.read_csv(labels_csv_path)\n",
        "\n",
        "# Sorted list of class names\n",
        "class_names = sorted(df['label'].unique().tolist())\n",
        "label_to_int = {c: i for i, c in enumerate(class_names)}\n",
        "int_to_label = {i: c for c, i in label_to_int.items()}\n",
        "\n",
        "df['label_id'] = df['label'].map(label_to_int)\n",
        "\n",
        "# Save class list for reuse\n",
        "class_json_path = os.path.join(WORK_DIR, 'class_names.json')\n",
        "with open(class_json_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(class_names, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Classes:\", class_names)\n",
        "print(\"Saved class_names.json at:\", class_json_path)\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "RMBBXDn1Cj5t",
        "outputId": "54867686-55eb-4f48-f4a2-d293fa81a9b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['Bio organico', 'Envase Plasticos', 'Papel y Carton', 'Reciclables Varios Cigarro', 'Reciclables Varios Electronicos', 'Reciclables Varios Madera', 'Reciclables Varios Metal', 'Reciclables Varios Otros', 'Reciclables Varios Plastico', 'Reciclables Varios Textiles', 'Reciclables Varios Vidrio', 'Todo lo demás']\n",
            "Saved class_names.json at: /content/drive/MyDrive/Colab Notebooks/ComputerVision/class_names.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          image_path           label  label_id\n",
              "0  /content/drive/MyDrive/Colab Notebooks/Compute...  Papel y Carton         2\n",
              "1  /content/drive/MyDrive/Colab Notebooks/Compute...  Papel y Carton         2\n",
              "2  /content/drive/MyDrive/Colab Notebooks/Compute...  Papel y Carton         2\n",
              "3  /content/drive/MyDrive/Colab Notebooks/Compute...  Papel y Carton         2\n",
              "4  /content/drive/MyDrive/Colab Notebooks/Compute...  Papel y Carton         2"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-99b065a1-3bbf-46a1-8994-529c348bdf91\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>label</th>\n",
              "      <th>label_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/Compute...</td>\n",
              "      <td>Papel y Carton</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/Compute...</td>\n",
              "      <td>Papel y Carton</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/Compute...</td>\n",
              "      <td>Papel y Carton</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/Compute...</td>\n",
              "      <td>Papel y Carton</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/Compute...</td>\n",
              "      <td>Papel y Carton</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-99b065a1-3bbf-46a1-8994-529c348bdf91')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-99b065a1-3bbf-46a1-8994-529c348bdf91 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-99b065a1-3bbf-46a1-8994-529c348bdf91');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-8d3fe3c8-8f3d-49c9-995c-4ea1cb744624\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8d3fe3c8-8f3d-49c9-995c-4ea1cb744624')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-8d3fe3c8-8f3d-49c9-995c-4ea1cb744624 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 59001,\n  \"fields\": [\n    {\n      \"column\": \"image_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 59001,\n        \"samples\": [\n          \"/content/drive/MyDrive/Colab Notebooks/ComputerVision/Dataset Consolidado/Reciclables Varios Metal/metal618.jpg\",\n          \"/content/drive/MyDrive/Colab Notebooks/ComputerVision/Dataset Consolidado/Envase Plasticos/plastic_937.jpg\",\n          \"/content/drive/MyDrive/Colab Notebooks/ComputerVision/Dataset Consolidado/Papel y Carton/cardboard_840.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"Reciclables Varios Textiles\",\n          \"Reciclables Varios Vidrio\",\n          \"Papel y Carton\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 11,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          9,\n          10,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Train/Validation Split on Original Dataset\n",
        "\n",
        "We split the original images into training and validation sets, stratified by class (keeping class proportions).\n",
        "Only training images will be augmented; validation images stay clean."
      ],
      "metadata": {
        "id": "Q0osVq-cCHs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "VAL_SPLIT = 0.2\n",
        "\n",
        "rng = np.random.default_rng(SEED)\n",
        "\n",
        "train_idx = []\n",
        "val_idx = []\n",
        "\n",
        "# Stratified split by label_id\n",
        "for cls, g in df.groupby('label_id', sort=False):\n",
        "    idx = g.index.to_numpy()\n",
        "    rng.shuffle(idx)\n",
        "    n_val = int(len(idx) * VAL_SPLIT)\n",
        "    val_idx.extend(idx[:n_val])\n",
        "    train_idx.extend(idx[n_val:])\n",
        "\n",
        "df_train_orig = df.loc[train_idx].reset_index(drop=True)\n",
        "df_val_orig   = df.loc[val_idx].reset_index(drop=True)\n",
        "\n",
        "print(\"Train originals:\", len(df_train_orig))\n",
        "print(\"Val originals:\", len(df_val_orig))\n",
        "\n",
        "# Optionally save original splits (no augmentation yet)\n",
        "df_train_orig.to_csv(os.path.join(WORK_DIR, 'original_train.csv'), index=False)\n",
        "df_val_orig.to_csv(os.path.join(WORK_DIR, 'original_val.csv'), index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5N0QU6VCGUw",
        "outputId": "3f20d612-fe6f-43f1-e4fb-ecfc22c53100"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train originals: 47205\n",
            "Val originals: 11796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.  OFFLINE Data Augmentation and training dataset pipeline generation to reuse later in model training\n"
      ],
      "metadata": {
        "id": "PkCyiCCqA4xe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Define Separate Augmentations (flip-only, rotate-only, etc.)\n",
        "We define individual augmentation operations:\n",
        "\n",
        "flip-only (horizontal)\n",
        "\n",
        "rotation-only\n",
        "\n",
        "zoom-only\n",
        "\n",
        "contrast-only (custom)\n",
        "\n",
        "salt-and-pepper-only (custom)\n",
        "Each original training image will generate one image per augmentation type, plus a copy of the original."
      ],
      "metadata": {
        "id": "oMIBuqfEDPb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import load_img, img_to_array, array_to_img\n",
        "import cv2\n",
        "\n",
        "IMG_SIZE = (224, 224)\n",
        "\n",
        "# ImageDataGenerator-based augmenters (one transform each)\n",
        "flip_gen = ImageDataGenerator(horizontal_flip=True)\n",
        "rot_gen  = ImageDataGenerator(rotation_range=15)   # ±15°\n",
        "zoom_gen = ImageDataGenerator(zoom_range=0.20)     # ±20% zoom\n",
        "\n",
        "def adjust_contrast(img, factor=1.5):\n",
        "    \"\"\"Contrast-only augmentation (RGB uint8 image).\"\"\"\n",
        "    lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    l = np.clip(l.astype(np.float32) * factor, 0, 255).astype(np.uint8)\n",
        "    enhanced = cv2.merge([l, a, b])\n",
        "    return cv2.cvtColor(enhanced, cv2.COLOR_LAB2RGB)\n",
        "\n",
        "def salt_and_pepper(image, amount=0.005):\n",
        "    \"\"\"Salt-and-pepper noise only on the image.\"\"\"\n",
        "    noisy = image.copy()\n",
        "    num_salt = int(np.ceil(amount * image.size * 0.5))\n",
        "    num_pepper = int(np.ceil(amount * image.size * 0.5))\n",
        "\n",
        "    # Salt (white)\n",
        "    coords = [np.random.randint(0, i - 1, num_salt) for i in image.shape[:2]]\n",
        "    noisy[coords[0], coords[1], :] = 255\n",
        "\n",
        "    # Pepper (black)\n",
        "    coords = [np.random.randint(0, i - 1, num_pepper) for i in image.shape[:2]]\n",
        "    noisy[coords[0], coords[1], :] = 0\n",
        "\n",
        "    return noisy\n"
      ],
      "metadata": {
        "id": "RaOLFXvAC6Yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Offline Augmentation (Training Set Only) into pablos folder\n",
        "\n",
        "Now we create an offline augmented training dataset:\n",
        "For each original training image we save:\n",
        "\n",
        "1 original copy\n",
        "\n",
        "1 flip-only image\n",
        "\n",
        "1 rotation-only image\n",
        "\n",
        "1 zoom-only image\n",
        "\n",
        "1 contrast-only image\n",
        "\n",
        "1 salt-and-pepper-only image\n",
        "All images are saved in Drive folder under Offline_Dataset_Augmented/<label>/...."
      ],
      "metadata": {
        "id": "93q9O8lpDWrf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "AUG_DATASET_DIR = os.path.join(PABLO_DIR, 'Offline_Dataset_Augmented')\n",
        "os.makedirs(AUG_DATASET_DIR, exist_ok=True)\n",
        "print(\"Augmented dataset will be stored at:\", AUG_DATASET_DIR)\n",
        "\n",
        "aug_train_records = []\n",
        "\n",
        "for idx, row in df_train_orig.iterrows():\n",
        "    orig_path = row['image_path']\n",
        "    label = row['label']\n",
        "\n",
        "    # Load original image and resize\n",
        "    img = load_img(orig_path, target_size=IMG_SIZE)\n",
        "    img_array = img_to_array(img).astype(\"uint8\")\n",
        "\n",
        "    # Create label folder in Pablo's Drive\n",
        "    out_dir = os.path.join(AUG_DATASET_DIR, label)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # 1) original copy\n",
        "    orig_name = f\"orig_{idx}.jpg\"\n",
        "    orig_save = os.path.join(out_dir, orig_name)\n",
        "    array_to_img(img_array).save(orig_save)\n",
        "    aug_train_records.append((orig_save, label))\n",
        "\n",
        "    # 2) flip-only\n",
        "    flipped = next(flip_gen.flow(np.expand_dims(img_array, 0), batch_size=1))[0].astype(\"uint8\")\n",
        "    flip_name = f\"flip_{idx}.jpg\"\n",
        "    flip_save = os.path.join(out_dir, flip_name)\n",
        "    array_to_img(flipped).save(flip_save)\n",
        "    aug_train_records.append((flip_save, label))\n",
        "\n",
        "    # 3) rotation-only\n",
        "    rotated = next(rot_gen.flow(np.expand_dims(img_array, 0), batch_size=1))[0].astype(\"uint8\")\n",
        "    rot_name = f\"rot_{idx}.jpg\"\n",
        "    rot_save = os.path.join(out_dir, rot_name)\n",
        "    array_to_img(rotated).save(rot_save)\n",
        "    aug_train_records.append((rot_save, label))\n",
        "\n",
        "    # 4) zoom-only\n",
        "    zoomed = next(zoom_gen.flow(np.expand_dims(img_array, 0), batch_size=1))[0].astype(\"uint8\")\n",
        "    zoom_name = f\"zoom_{idx}.jpg\"\n",
        "    zoom_save = os.path.join(out_dir, zoom_name)\n",
        "    array_to_img(zoomed).save(zoom_save)\n",
        "    aug_train_records.append((zoom_save, label))\n",
        "\n",
        "    # 5) contrast-only\n",
        "    contrast_img = adjust_contrast(img_array, factor=1.5)\n",
        "    contrast_name = f\"contrast_{idx}.jpg\"\n",
        "    contrast_save = os.path.join(out_dir, contrast_name)\n",
        "    array_to_img(contrast_img).save(contrast_save)\n",
        "    aug_train_records.append((contrast_save, label))\n",
        "\n",
        "    # 6) salt-and-pepper-only\n",
        "    sp_img = salt_and_pepper(img_array, amount=0.005)\n",
        "    sp_name = f\"sp_{idx}.jpg\"\n",
        "    sp_save = os.path.join(out_dir, sp_name)\n",
        "    array_to_img(sp_img).save(sp_save)\n",
        "    aug_train_records.append((sp_save, label))\n",
        "\n",
        "print(\"Number of augmented+original train images:\", len(aug_train_records))\n",
        "\n"
      ],
      "metadata": {
        "id": "hs2jX0wVMLoi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We initially considered many offline augmentations, but due to time and storage constraints we focused on the most realistic and impactful ones for waste classification: horizontal flip, small rotations, and small zoom. These reflect how a user might photograph waste in different orientations and distances. We removed very heavy transforms like strong noise and standalone contrast changes, which are less realistic and significantly increased preprocessing time"
      ],
      "metadata": {
        "id": "v7RtI2w1LEaP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###FAST offline augmentation code (3 images per original)"
      ],
      "metadata": {
        "id": "2VYe8xLiLS7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the dataset contained over 47,000 training images, generating all augmentation types separately would require over 72 hours of compute time. To respect time constraints, we selected the two augmentations that are considered most important and realistic for waste classification: horizontal flip and small rotation. These cover common real-world variations while keeping processing feasible.\n",
        "\n",
        "Expected runtime with this plan\n",
        "\n",
        "Using only flip + rotation:\n",
        "\n",
        "Processing per image = 1/3 of what we had before\n",
        "\n",
        "Predictions:\n",
        "\n",
        "~47k flips\n",
        "\n",
        "~47k rotations\n",
        "\n",
        "~47k original saves\n",
        "\n",
        "This is approximately:\n",
        "\n",
        "~141k images\n",
        "\n",
        "~10–12 hours total on Google Drive"
      ],
      "metadata": {
        "id": "tLHsXzmxLXHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AUG_DATASET_DIR = os.path.join(PABLO_DIR, 'Offline_Dataset_Augmented')\n",
        "os.makedirs(AUG_DATASET_DIR, exist_ok=True)\n",
        "print(\"Augmented dataset will be stored at:\", AUG_DATASET_DIR)\n",
        "\n",
        "aug_train_records = []\n",
        "\n",
        "for idx, row in df_train_orig.iterrows():\n",
        "    orig_path = row['image_path']\n",
        "    label = row['label']\n",
        "\n",
        "    # Load original image and resize\n",
        "    img = load_img(orig_path, target_size=IMG_SIZE)\n",
        "    img_array = img_to_array(img).astype(\"uint8\")\n",
        "\n",
        "    # Create label folder\n",
        "    out_dir = os.path.join(AUG_DATASET_DIR, label)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # 1) original copy\n",
        "    orig_name = f\"orig_{idx}.jpg\"\n",
        "    orig_save = os.path.join(out_dir, orig_name)\n",
        "    array_to_img(img_array).save(orig_save)\n",
        "    aug_train_records.append((orig_save, label))\n",
        "\n",
        "    # 2) flip-only\n",
        "    flipped = next(flip_gen.flow(np.expand_dims(img_array, 0), batch_size=1))[0].astype(\"uint8\")\n",
        "    flip_name = f\"flip_{idx}.jpg\"\n",
        "    flip_save = os.path.join(out_dir, flip_name)\n",
        "    array_to_img(flipped).save(flip_save)\n",
        "    aug_train_records.append((flip_save, label))\n",
        "\n",
        "    # 3) rotation-only\n",
        "    rotated = next(rot_gen.flow(np.expand_dims(img_array, 0), batch_size=1))[0].astype(\"uint8\")\n",
        "    rot_name = f\"rot_{idx}.jpg\"\n",
        "    rot_save = os.path.join(out_dir, rot_name)\n",
        "    array_to_img(rotated).save(rot_save)\n",
        "    aug_train_records.append((rot_save, label))\n",
        "\n",
        "print(\"Total saved images:\", len(aug_train_records))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCIq-KNDLHr8",
        "outputId": "3c9df71b-8a0d-4c79-ef46-8811c242ad5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Augmented dataset will be stored at: /content/drive/MyDrive/[01] - Pablo/[01].[02] - Facultad/[01].[02].[05] - Master Fing/Computer Vision/Project/Entrega Reciclaje/Offline_Dataset_Augmented\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "image file is truncated (26 bytes not processed)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3110507435.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Load original image and resize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mimg_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uint8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/image_utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth_height_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcrop_box\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth_height_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2299\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2301\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreducing_gap\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresample\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mResampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEAREST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    385\u001b[0m                                         \u001b[0;34mf\"({len(b)} bytes not processed)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                                     )\n\u001b[0;32m--> 387\u001b[0;31m                                     \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: image file is truncated (26 bytes not processed)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Build Final Train/Val CSVs\n",
        "We build the final training and validation tables for model training:\n",
        "\n",
        "offline_aug_train.csv: original + augmented training images.\n",
        "\n",
        "offline_aug_val.csv: clean original validation images."
      ],
      "metadata": {
        "id": "Pu8_FgpHDg8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_final = pd.DataFrame(aug_train_records, columns=['image_path', 'label'])\n",
        "df_val_final   = df_val_orig[['image_path', 'label']].copy()\n",
        "\n",
        "train_csv_path = os.path.join(WORK_DIR, 'offline_aug_train.csv')\n",
        "val_csv_path   = os.path.join(WORK_DIR, 'offline_aug_val.csv')\n",
        "\n",
        "df_train_final.to_csv(train_csv_path, index=False)\n",
        "df_val_final.to_csv(val_csv_path, index=False)\n",
        "\n",
        "print(\"Saved offline_aug_train.csv:\", len(df_train_final))\n",
        "print(\"Saved offline_aug_val.csv:\", len(df_val_final))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Ilx33jzVEV5p",
        "outputId": "fbf6abb8-1ccb-435c-fd49-c5a3880d66ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'aug_train_records' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3654021151.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_train_records\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_path'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_val_final\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mdf_val_orig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_path'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_csv_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWORK_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'offline_aug_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_csv_path\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWORK_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'offline_aug_val.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'aug_train_records' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Visualize Some Augmented Images\n",
        "Quick sanity check: show a few randomly chosen augmented images from a given class to verify that augmentation worked as expected."
      ],
      "metadata": {
        "id": "qEH3u61qDtFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_augmented_samples(label, n=6):\n",
        "    folder = os.path.join(AUG_DATASET_DIR, label)\n",
        "    image_files = os.listdir(folder)\n",
        "    chosen = random.sample(image_files, min(n, len(image_files)))\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    for i, name in enumerate(chosen):\n",
        "        path = os.path.join(folder, name)\n",
        "        img = cv2.imread(path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        plt.subplot(2, (n+1)//2, i+1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(name[:12] + \"...\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.suptitle(f\"Augmented samples for: {label}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example: view some plastic images (change label as needed)\n",
        "show_augmented_samples('Reciclables Varios Plastico')\n"
      ],
      "metadata": {
        "id": "C6lCGFsADwq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ResNet50 with OFFLINE data augmentation"
      ],
      "metadata": {
        "id": "H7Sc_kyyD-Xd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Build tf.data Datasets (Train = Augmented, Val = Clean)\n",
        "We load the offline-augmented training set and the clean validation set with tf.data.\n",
        "No further augmentation is done here, because the augmentation is already baked into the training images."
      ],
      "metadata": {
        "id": "ojPCJzhfD0Zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "df_train = pd.read_csv(train_csv_path)\n",
        "df_val   = pd.read_csv(val_csv_path)\n",
        "\n",
        "# Encode labels\n",
        "class_names = sorted(df_train['label'].unique())\n",
        "label_to_int = {c: i for i, c in enumerate(class_names)}\n",
        "df_train['label_id'] = df_train['label'].map(label_to_int)\n",
        "df_val['label_id']   = df_val['label'].map(label_to_int)\n",
        "\n",
        "def load_image(path, label_id):\n",
        "    img = tf.io.decode_image(tf.io.read_file(path), channels=3, expand_animations=False)\n",
        "    img = tf.image.resize(img, IMG_SIZE)\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    return img, label_id\n",
        "\n",
        "train_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((df_train['image_path'], df_train['label_id']))\n",
        "    .shuffle(8000)\n",
        "    .map(load_image, num_parallel_calls=AUTOTUNE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTOTUNE)\n",
        ")\n",
        "\n",
        "val_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((df_val['image_path'], df_val['label_id']))\n",
        "    .map(load_image, num_parallel_calls=AUTOTUNE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTOTUNE)\n",
        ")\n",
        "\n",
        "print(\"Datasets ready.\")\n"
      ],
      "metadata": {
        "id": "4ynb3cIvD0JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Compute Class Weights"
      ],
      "metadata": {
        "id": "LrF_5-oKEDe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "counts = Counter(df_train['label_id'])\n",
        "total = sum(counts.values())\n",
        "class_weight = {cls: total / (len(counts) * cnt) for cls, cnt in counts.items()}\n",
        "\n",
        "print(\"Class counts:\", counts)\n",
        "print(\"Class weights:\", class_weight)\n"
      ],
      "metadata": {
        "id": "HOrUwwNQEchu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Define Callbacks"
      ],
      "metadata": {
        "id": "QqVf8MwHEdGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_dir = os.path.join(WORK_DIR, 'checkpoints_resnet50')\n",
        "log_dir  = os.path.join(WORK_DIR, 'logs_resnet50')\n",
        "os.makedirs(ckpt_dir, exist_ok=True)\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(ckpt_dir, 'best.keras'),\n",
        "        monitor='val_accuracy',\n",
        "        mode='max',\n",
        "        save_best_only=True\n",
        "    ),\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=2\n",
        "    ),\n",
        "    tf.keras.callbacks.TensorBoard(log_dir=log_dir),\n",
        "]\n"
      ],
      "metadata": {
        "id": "r3ZPU9pBEfdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. ResNet-50 Phase 1: Train Only the Classifier Head"
      ],
      "metadata": {
        "id": "m3bLEM7xEiv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# Pretrained ResNet50 backbone\n",
        "base = tf.keras.applications.ResNet50(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    input_shape=(*IMG_SIZE, 3)\n",
        ")\n",
        "base.trainable = False  # freeze backbone\n",
        "\n",
        "inputs = layers.Input(shape=(*IMG_SIZE, 3))\n",
        "x = base(inputs, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history1 = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=10,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weight\n",
        ")\n"
      ],
      "metadata": {
        "id": "A2hGFLm6EkrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. ResNet-50 Phase 2: Fine-Tune the Top Layers"
      ],
      "metadata": {
        "id": "2L_k0RgcEmos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze last ~30 layers of the backbone\n",
        "fine_tune_at = len(base.layers) - 30\n",
        "\n",
        "for i, layer in enumerate(base.layers):\n",
        "    layer.trainable = (i >= fine_tune_at)\n",
        "\n",
        "print(\"Trainable backbone layers in Phase 2:\",\n",
        "      sum(layer.trainable for layer in base.layers))\n",
        "\n",
        "# Re-compile with a smaller learning rate\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history2 = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=10,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weight\n",
        ")\n"
      ],
      "metadata": {
        "id": "IhnLxsc3EnkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Final Evaluation (Accuracy, Confusion Matrix, Report)"
      ],
      "metadata": {
        "id": "XYdGyef6EqRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ground-truth labels\n",
        "y_true = df_val['label_id'].values\n",
        "\n",
        "# Predictions\n",
        "y_pred_probs = model.predict(val_ds)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "print(\"Validation accuracy:\", np.mean(y_true == y_pred))\n",
        "\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix - Validation Set\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lwSi3ToHEq_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Prediction Function for New Images\n"
      ],
      "metadata": {
        "id": "wPDqdCv0JCHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_single_image(image_path):\n",
        "    img = tf.io.decode_image(tf.io.read_file(image_path), channels=3, expand_animations=False)\n",
        "    img = tf.image.resize(img, IMG_SIZE)\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    return img\n",
        "\n",
        "def predict_image(image_path, model, class_names, top_k=3):\n",
        "    img = preprocess_single_image(image_path)\n",
        "    img_batch = tf.expand_dims(img, 0)\n",
        "\n",
        "    preds = model.predict(img_batch)[0]\n",
        "    top_indices = np.argsort(preds)[::-1][:top_k]\n",
        "    top_probs = preds[top_indices]\n",
        "    top_labels = [class_names[i] for i in top_indices]\n",
        "\n",
        "    print(f\"Predictions for: {image_path}\")\n",
        "    for lbl, prob in zip(top_labels, top_probs):\n",
        "        print(f\"  {lbl}: {prob:.4f}\")\n",
        "\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Top-1: {top_labels[0]} ({top_probs[0]:.2f})\")\n",
        "    plt.show()\n",
        "\n",
        "# Example:\n",
        "# predict_image('/content/drive/MyDrive/some_image.jpg', model, class_names)\n"
      ],
      "metadata": {
        "id": "Mz9uvU6fJEl6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}